{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lazargugleta/mg-ml-genai-interview?scriptVersionId=260608180\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import polars\nimport torch\nfrom dataclasses import dataclass\nfrom huggingface_hub import login as hf_login\n# from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nimport io\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models import resnet50\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import List\nimport sys\nimport logging\n\n# setting the logger\nlogger = logging.getLogger(__name__)\n\n# special for Kaggle\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\nlogging.basicConfig(stream = sys.stdout, level=logging.INFO)\nlogger.setLevel(logging.INFO)","metadata":{"id":"9w2UgSWR-GJ4","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:23.779084Z","iopub.execute_input":"2025-09-08T08:06:23.779392Z","iopub.status.idle":"2025-09-08T08:06:34.799696Z","shell.execute_reply.started":"2025-09-08T08:06:23.779367Z","shell.execute_reply":"2025-09-08T08:06:34.798851Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def login_hf(hf_token: str):\n  \"\"\"\n  Helper function to log in Huggingface with proper error handling.\n  \"\"\"\n\n  if hf_token is not None: # clear setting of case checking\n      hf_login(hf_token)\n      logger.info(\"Successfully logged in to Hugging Face!\")  # I would use a logging function of python in production to control levels of information\n  else:\n      logger.exception(\"Token is not set. Please save the token first.\") # clear message and letting know the user the issue","metadata":{"id":"X8G08WWvDHtf","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:34.801214Z","iopub.execute_input":"2025-09-08T08:06:34.801562Z","iopub.status.idle":"2025-09-08T08:06:34.806803Z","shell.execute_reply.started":"2025-09-08T08:06:34.801541Z","shell.execute_reply":"2025-09-08T08:06:34.805684Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Preprocessing steps","metadata":{"id":"kKA3FQ3hvGAM"}},{"cell_type":"code","source":"def get_df_split() -> List[polars.DataFrame]:\n  \"\"\"\n  This could be further improved by setting percentage parameter between\n  train/validation split. No need here as dataset is prepared by huggingface.\n  We should always put return types.\n  \"\"\"\n  splits = {'train': 'data/train-*.parquet', 'validation': 'data/validation-*.parquet'}\n  df_train = polars.read_parquet('hf://datasets/ethz/food101/' + splits['train'])\n  df_validation = polars.read_parquet('hf://datasets/ethz/food101/' + splits['validation'])\n  return [df_train, df_validation]\n\n\ndef _unnest_and_select_columns(df_train: polars.DataFrame, df_validation: polars.DataFrame) -> List[polars.DataFrame]:\n  df_train_preprocessed = df_train.unnest('image')\n  df_validation_preprocessed = df_validation.unnest('image')\n  df_train_preprocessed = df_train_preprocessed.select(\n      polars.col('bytes'),\n      # considered categorical also, but not suitable for ML usage\n      polars.col('label').cast(polars.Int8)\n  )\n  df_validation_preprocessed = df_validation_preprocessed.select(\n      polars.col('bytes'),\n      polars.col('label').cast(polars.Int8)\n  )\n  return [df_train_preprocessed, df_validation_preprocessed]\n\ndef _downsample_df(df, n_samples) -> polars.DataFrame:\n  \"\"\"\n  Downsample polars dataframe to a certain number of samples.\n\n  Perfect example of a testable function.\n  Does one thing with clearly set parameters.\n  We want to test behaviour so writing test names clearly can improve code clarity.\n  Method can be extended to choose which classes to leave in.\n  \"\"\"\n  downsampled_dfs = []\n  for label in range(1, 6):  # First five classes just for this example\n      class_df = df.filter(polars.col(\"label\") == label)\n      sampled_class_df = class_df.sample(n=n_samples)\n      downsampled_dfs.append(sampled_class_df)\n  return polars.concat(downsampled_dfs)\n\n\nclass PolarsImageDataset(Dataset):\n  \"\"\"\n  Make a class based on DataSet parent class, extended with functions to\n  make polars work with this model.\n  Also possible to use to_torch() method if dataframe is set up properly before.\n  \"\"\"\n  def __init__(self, polars_df):\n      self.df = polars_df\n      self.images = polars_df[\"bytes\"].to_list()\n      self.labels = polars_df[\"label\"].to_list()\n      self.resize_and_normalize = transforms.Compose([\n          transforms.Resize((224, 224)),\n          transforms.ToTensor(),\n          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n      ])\n\n  def _binary_to_tensor(self, binary_data):\n    image = Image.open(io.BytesIO(binary_data)).convert('RGB')\n    return self.resize_and_normalize(image)\n\n  def __len__(self):\n      return len(self.df)\n\n  def __getitem__(self, idx):\n      image_tensor = self._binary_to_tensor(self.images[idx])\n      label = self.labels[idx] - 1  # Convert to 0-indexed if needed\n      return image_tensor, label\n\n\ndef _create_polars_image_datasets(df_train: polars.DataFrame, df_validation: polars.DataFrame) -> List[PolarsImageDataset]:\n  train_dataset = PolarsImageDataset(df_train)\n  test_dataset = PolarsImageDataset(df_validation)\n  return [train_dataset, test_dataset]\n\n\ndef _create_dataloaders(train_dataset: PolarsImageDataset, test_dataset: PolarsImageDataset, batch_size: polars.Int32) -> List[DataLoader]:\n  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n  return [train_loader, test_loader]\n\n\ndef preprocessing(df_train: polars.DataFrame, df_validation: polars.DataFrame, batch_size: polars.Int32) -> List[DataLoader]:\n  \"\"\"\n  Steps taken to preprocess are that image field is a struct so it should be\n  unnested and label is integer64 for 101 classes hence we can also reduce the\n  memory it takes to store it.\n  We leave path field out after unnesting.\n  This function is like an abstraction of underlying preprocessing steps.\n  \"\"\"\n\n  df_train_preprocessed, df_validation_preprocessed = _unnest_and_select_columns(df_train, df_validation)\n\n  df_train_downsampled = _downsample_df(df_train_preprocessed, 750)\n  df_validation_downsampled = _downsample_df(df_validation_preprocessed, 250)\n\n  train_dataset, test_dataset = _create_polars_image_datasets(df_train_downsampled, df_validation_downsampled)\n  train_loader, test_loader = _create_dataloaders(train_dataset, test_dataset, batch_size=64)\n\n  return [train_loader, test_loader]","metadata":{"id":"GCaIxblnk9qc","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:34.807905Z","iopub.execute_input":"2025-09-08T08:06:34.808255Z","iopub.status.idle":"2025-09-08T08:06:34.829284Z","shell.execute_reply.started":"2025-09-08T08:06:34.808223Z","shell.execute_reply":"2025-09-08T08:06:34.828139Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Configure and train model","metadata":{"id":"PLB8oebVDXUd"}},{"cell_type":"code","source":"@dataclass\nclass ModelConfiguration:\n    \"\"\"Configuration for model training\"\"\"\n    model_name: str = \"resnet50\"\n    num_classes: int = 5\n    weights: str = \"IMAGENET1K_V1\"\n    learning_rate: float = 0.001\n    weight_decay: float = 0.000\n    optimizer_class: type = optim.Adam\n    criterion_class: type = nn.CrossEntropyLoss\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n\ndef _create_pretrained_resnet50_model_and_components(model_config: ModelConfiguration):\n  \"\"\"\n  Use a pre-trained model and fine tune on our prepared dataset.\n\n  Good name for a function is it's best documentation:\n  https://www.oreilly.com/library/view/the-rules-of/9781098133108/ch03.html\n  \"\"\"\n  if model_config.model_name == \"resnet50\":\n    model = resnet50(weights=model_config.weights)\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, model_config.num_classes)\n  else:\n    raise ValueError(f\"Unsupported model: {model_config.model_name}\")\n\n  device = torch.device(model_config.device)\n  model = model.to(device)\n  criterion = model_config.criterion_class()\n  optimizer = model_config.optimizer_class(model.parameters(), lr=model_config.learning_rate, weight_decay=model_config.weight_decay)\n  return device, model, criterion, optimizer\n\ndef train_model(model_config: ModelConfiguration, train_loader,/,*, num_epochs=20):\n  \"\"\"\n  Training a model in a certain number of epochs.\n\n  At first I was passing all return parameters of _create_pretrained_resnet50_model_and_components\n  throughout the pipeline but that does not make sense as it is making code less readable.\n  Solution here is to make a configuration class to keep track of all model settings.\n  Could be improved further to have train and evaluate methods part of a bigger class.\n  \"First make it work, make it right, then make it fast\".\n  \"\"\"\n  device, model, criterion, optimizer = _create_pretrained_resnet50_model_and_components(model_config)\n    \n  for epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    logger.info(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n      \n  return model\n\ndef evaluate_model(model, test_loader: DataLoader):\n  \"\"\"\n  Calculate accuracy, recall and f1 scores based on the validation set using\n  the trained model.\n  \"\"\" \n    \n  device = next(model.parameters()).device\n  model.eval()\n  all_preds = []\n  all_labels = []\n\n  with torch.no_grad():\n      for inputs, labels in test_loader:\n          inputs, labels = inputs.to(device), labels.to(device)\n          outputs = model(inputs)\n          _, preds = torch.max(outputs, 1)\n\n          all_preds.extend(preds.cpu().numpy())\n          all_labels.extend(labels.cpu().numpy())\n\n  accuracy = accuracy_score(all_labels, all_preds)\n  logger.info(f\"Test Accuracy: {accuracy:.4f}\")\n  logger.info(classification_report(all_labels, all_preds))\n  return accuracy, all_preds, all_labels","metadata":{"id":"CaTbp4_MQrqm","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:34.831213Z","iopub.execute_input":"2025-09-08T08:06:34.831479Z","iopub.status.idle":"2025-09-08T08:06:34.857353Z","shell.execute_reply.started":"2025-09-08T08:06:34.831456Z","shell.execute_reply":"2025-09-08T08:06:34.856513Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Run the prototype","metadata":{"id":"PErHgUGeWrsy"}},{"cell_type":"code","source":"def main():\n  \"\"\"\n  Abstraction of all methods with clear modular steps.\n  It can be improved further.\n  \"\"\"\n  user_secrets = UserSecretsClient()\n  HF_TOKEN=user_secrets.get_secret(\"HF_TOKEN\")\n  login_hf(HF_TOKEN)\n  df_train, df_validation = get_df_split()\n  train_loader, test_loader = preprocessing(df_train, df_validation, batch_size=64)\n  model_config = ModelConfiguration(num_classes=5, learning_rate=0.001)\n  # device, model, criterion, optimizier = create_pretrained_resnet50_model_and_components()\n  trained_model = train_model(model_config, train_loader, num_epochs=20)\n  accuracy, all_preds, all_labels = evaluate_model(trained_model, test_loader)\n  return accuracy, all_preds, all_labels","metadata":{"id":"krCf_JPkpqp_","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:34.858189Z","iopub.execute_input":"2025-09-08T08:06:34.858435Z","iopub.status.idle":"2025-09-08T08:06:34.874191Z","shell.execute_reply.started":"2025-09-08T08:06:34.858409Z","shell.execute_reply":"2025-09-08T08:06:34.873141Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"accuracy, all_preds, all_labels = main()","metadata":{"id":"ZbT-s0I8spJb","outputId":"a73ecad2-bf2f-4213-8d42-d1170033a9c3","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:34.875321Z","iopub.execute_input":"2025-09-08T08:06:34.875581Z","execution_failed":"2025-09-08T08:06:44.159Z"}},"outputs":[{"name":"stdout","text":"INFO:__main__:Successfully logged in to Hugging Face!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Analyze results per each class","metadata":{"id":"eYWMxw7hm7JP"}},{"cell_type":"code","source":"def create_confusion_matrix(all_preds, all_labels):\n  cm = confusion_matrix(all_labels, all_preds)\n\n  disp = ConfusionMatrixDisplay(cm)\n  fig, ax = plt.subplots(figsize=(10, 8))\n  disp.plot(ax=ax, cmap='Blues', values_format='d')\n  plt.xticks(rotation=45)\n  plt.title('Confusion Matrix')\n  plt.tight_layout()\n  plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n  plt.show()\n\ncreate_confusion_matrix(all_preds, all_labels)","metadata":{"id":"xKQa_CQk4iz6","trusted":true,"execution":{"execution_failed":"2025-09-08T08:06:44.159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{"id":"ea4ykWBFmg_6"}},{"cell_type":"code","source":"def hyperparameter_tuning():\n  \"\"\"\n  To optimize model parameters we use hypertuning technique\n  and find out best parameters to produce best results.\n  \"\"\"\n  learning_rates = [1e-3, 1e-4, 5e-5]\n  weight_decays = [0, 1e-4, 1e-3]\n  batch_sizes = [32, 64]\n\n  best_accuracy = 0\n  best_params = {}\n\n  for lr in learning_rates:\n      for wd in weight_decays:\n          for bs in batch_sizes:\n              logger.info(f\"Testing lr: {lr}, wd: {wd}, bs: {bs}\")\n              df_train, df_validation = get_df_split()\n              train_loader, test_loader = preprocessing(df_train, df_validation, batch_size=bs)\n              # device, model, criterion, optimizer = create_pretrained_resnet50_model_and_components()\n              model_config = ModelConfiguration(num_classes=5, learning_rate=lr, weight_decay=wd)\n              trained_model = train_model(model_config, train_loader, num_epochs=20)\n              accuracy, _, _ = evaluate_model(trained_model, test_loader)\n              if accuracy > best_accuracy:\n                  best_accuracy = accuracy\n                  best_params = {'lr': lr, 'wd': wd, 'bs': bs}\n              logger.info(f\"Accuracy: {accuracy:.4f}\\n\")\n\n  logger.info(f\"Best parameters: {best_params}\")\n  logger.info(f\"Best validation accuracy: {best_accuracy:.4f}\")\n  return best_params","metadata":{"id":"hMS9ldTciRFn","trusted":true,"execution":{"execution_failed":"2025-09-08T08:06:44.16Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = hyperparameter_tuning()","metadata":{"id":"JTisxhHbJ6_Q","trusted":true,"execution":{"execution_failed":"2025-09-08T08:06:44.16Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the best hyperparameters to train your final model\nbest_lr = best_params['lr']\nbest_wd = best_params['wd']\nbest_bs = best_params['bs']\n\ndf_train, df_validation = get_df_split()\ntrain_loader, test_loader = preprocessing(df_train, df_validation, batch_size=best_bs)\n\n# Initialize model and optimizer\nmodel_config = ModelConfiguration(learning_rate=best_lr, weight_decay=best_wd)\ntrained_model = train_model(model_config, train_loader, num_epochs=2)\n\nfinal_accuracy, _, _ = evaluate_model(trained_model, test_loader)\nlogger.info(f\"Final test accuracy with tuned parameters: {final_accuracy:.4f}\")","metadata":{"id":"kT-nlryvjR_B","trusted":true,"execution":{"execution_failed":"2025-09-08T08:06:44.16Z"}},"outputs":[],"execution_count":null}]}